---
title: 初学深度学习笔记——优化算法和学习率调度器
date: 2024/7/25
type: deep_learning
---

[[toc]]

主要介绍冲量法和 Adam 算法的优化思想，以及调整学习率的一些策略。

## 优化算法

介绍这两种算法之前，先来看看在小批量 $B$ 下的小批量随机梯度下降中的权重更新方式：
$$
g_t=\frac{1}{B_t}\sum_{i\in B_t}\bigtriangledown l_i(x_{t-1}),
$$

$$
w_t=w_{t-1}-\eta g_t.
$$

### 冲量法

冲量法使用平滑过的梯度对权重进行更新：
$$
v_t=\beta v_{t-1}+g_t,
$$

$$
w_t=w_{t-1}-\eta v_t.
$$

其中$\beta\in(0,1)$。 这种做法有效地将瞬时梯度替换为多个“过去”梯度的平均值。$v$被称为*动量*（momentum），它累加了过去的梯度。为了更详细地解释，让我们递归地将$v_t$扩展到：
$$
v_t=g_t+\beta g_{t-1}+\beta^2g_{t-2}+\beta^3g_{t-3}+\dots.
$$
其中，$\beta$的常见取值有$[0.5,0.9,0.95,0.00]$，较大的$\beta$会取过去多个梯度的平均。新的梯度替换不再指向特定实例下降最陡的方向，而是指向**过去梯度的加权平均值**的方向。这使我们能够实现对单批量计算平均值的大部分好处，而不产生实际计算其梯度的代价。

### Adam

Adam 算法将许多优化算法的功能结合到了相当强大的更新规则中。

Adam 算法的关键组成部分之一是：它使用**指数加权移动平均值**来估算梯度的**动量和二次矩**，即它使用状态变量。
$$
v_t=\beta_1 v_{t-1}+(1-\beta_1)g_t,
$$

$$
s_t=\beta_2 s_{t-1}+(1-\beta_2)g_t^2.
$$

这里$\beta_1$和$\beta_2$是非负加权参数。常将它们设置为$\beta_1=0.9$和$\beta_2=0.999$。也就是说，方差估计的移动远远慢于动量估计的移动。

这里$g_t$和$g_t^2$都乘以了一个系数，原因在于这样可以使得过去所有梯度的权重之和为 $1$。由$v_t$递归扩展的公式可以得知，所有梯度的权重可以看作一个等比数列，则所有权重之和等于（对于$\beta_2$而言也是一样的结果，此处不再赘述）：
$$
\sum^t_{i=0}\beta^i_1=\frac{1-\beta^t_1}{1-\beta_1},
$$
当$t$趋近于无穷大时，由于$\beta_1$是一个小于 $1$ 的数，故可以近似看作：
$$
\sum^\infty_{i=0}\beta^i_1=\frac{1}{1-\beta_1}.
$$
因此，$g_t$乘以一个 $1-\beta_1$ 可以使得所有的权重之和为 $1$，从而保证权重不会过大。

但当$t=1$时，即$v_0=s_0=0$，此时权重之和就会分别等于 $1-\beta_1^t$ 和 $1-\beta_2^t$，是一个非常小的数，那么$v_t$和$s_t$所得到的值就会很小，导致一个相当大的初始偏差。为了解决这个问题，进行如下的修正：
$$
\hat v_t=\frac{v_t}{1-\beta_1^t}，\hat s_t=\frac{s_t}{1-\beta_2^t}.
$$
这样在$t$很小时，就可以把权重之和拉回到 $1$。

有了正确的估计，我们现在可以写出更新方程：
$$
g'_t=\frac{\hat v_t}{\sqrt{\hat s_t}+\epsilon}
$$
$g'_t$的计算方式可以使得最终计算得到的梯度在一个合适的范围内，不会太大也不会太小。

最后更新模型权重：
$$
w_t=w_{t-1}-\eta g'
$$

## 学习率调度器

### 单因子调度器

通过乘法衰减，每次迭代通过乘以一个系数来减小学习率，即$\eta_{t+1}=\eta_t\cdot\alpha$其中$\alpha\in(0,1)$。为了防止学习率衰减到一个合理的下界之下， 更新方程经常修改为$\eta_{t+1}=max(\eta_{min},\eta_t\cdot\alpha)$。

### 多因子调度器

常见策略之一是保持学习率为一组分段的常量，并且不时地按给定的参数对学习率做乘法衰减。具体地说，给定一组降低学习率的时间点，例如$s=[5,10,20]$，每当$t\in s$时，使得$\eta_{t+1}=\eta_t\cdot\alpha$。

这种分段恒定学习率调度背后的直觉是，让优化持续进行，直到权重向量的分布达到一个**驻点**。此时，我们才将学习率降低，以获得更高质量的代理**来达到一个良好的局部最小值**。

### 余弦调度器

*余弦调度器*的主要观点是：我们可能不想在一开始就太大地降低学习率，而且可能希望最终能用非常小的学习率来“改进”解决方案。这产生了一个类似于余弦的调度，函数形式如下所示，学习率的值在$t\in[0,T]$之间：
$$
\eta_t=\eta_T+\frac{\eta_0-\eta_T}{2}\left(1+cos(\frac{\pi t}{T})\right)
$$

### 预热

在某些情况下，初始化参数不足以得到良好的解。**一开始的大步更新可能没有好处**，特别是因为最初的参数集是随机的。**最初的更新方向可能也是毫无意义的**。这对某些高级网络设计来说尤其棘手，可能导致不稳定的优化结果。对此，一方面，我们可以选择一个足够小的学习率，从而防止一开始发散，然而这样进展太缓慢。另一方面，如果选择较高的学习率，那么最初就会导致发散。

解决这种问题的一个相当简单的解决方法是使用***预热***。在预热期间学习率将从较小的值逐步增加至初始的最大值，然后再进入调度器的优化过程。预热可以应用于任何调度器，从而防止发散。